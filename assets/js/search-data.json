{
  
    
        "post0": {
            "title": "Udacity Capstone Part 1 Definition And Data",
            "content": "A Machine Learning Engineer Nanodegree Capstone Project . Multi-Label Auto-Tagging of Audio Files Using fastai2 Audio . Part 1 - Definition and Data . Welcome to part 1 of a blog series based on my Udacity Machine Learning Engineer Nanodegree Capstone project. This initial section deals with the problem definition, outlines the solution approach using the in-development fastai2 audio library and discusses the dataset. . The blog series will be structured as follows: . Problem Definition, Proposed Solution and Data Exploration | Implementation | Results and Analysis | Links will be provided as the series progresses. Please see the associated GitHub repository for all notebooks, please contact me with any queries. . I. Problem Definition . Overview . The sub-field of Machine Learning known as Machine Listening is a burgeoning area of research using signal processing for the automatic extraction of information from sound by a computational analysis of audio. There are many different areas of research within this field as demonstrated by the latest Detection and Classification of Acoustic Scenes and Events (DCASE) 2020 Challenge1, a machine learning challenge dedicated to the research and development of new methods and algorithms for audio. These include: . Acoustic Scene Classification | Sound Event Detection and Localization | Sound Event Detection and Separation in Domestic Environments | Urban Sound tagging | Automated Audio Captioning | . As an acoustic engineer, I am extremely intrigued by this new field. Recent developments in machine learning algorithms have allowed significant progress to be made within this area, with the potential applications of the technology being wide and varied and meaning the tools could prove to be extremely useful for the acoustic practitioner amongst many other uses. . The in-development user-contributed fast.ai2 Audio library2 inspired me to undertake the development of a deep learning audio-tagging system for this Udacity Capstone project, described herein. . Problem Statement . The Freesound Audio Tagging 2019 Kaggle Competition provides the basis for my research project3. . The challenge is to develop a system for the automatic classification of multi-labelled audio files within 80 categories, that could potentially be used for automatic audio/video file tagging with noisy or untagged audio data. This has historically been investigated in a variety of ways: . Conversion of audio to mel-spectrogram images fed into CNNs | End-to-End Deep learning | Custom architectures involving auto-encoders | Features representation transfer learning with custom architectures and Google’s Audioset | . In addition, the classification of weakly labelled data from large-scale crowdsourced datasets provides a further problem for investigation4. The problem is clearly quantifiable in that a number of accuracy metrics could be used to quantify the accuracy of the model’s predictions, described below. . The competition dataset comprises audio clips from the following existing datasets: . “Curated Train Set” - Freesound Dataset (FSD): a smaller dataset collected at the MTG-UPF based on Freesound content organized with the AudioSet Ontology and manually labelled by humans. 4964 files. | “Noisy Train Set” - The soundtracks of a pool of Flickr videos taken from the Yahoo Flickr Creative Commons 100M dataset (YFCC) which are automatically labelled using metadata from the original Flickr clips. These items therefore have significantly more label noise than the Freesound Dataset items. 19815 files. | . The data comprises 80 categories labelled according to Google’s Audioset Ontology 3 with ground truth labels provided at the clip level. The clips range in duration between 0.3 to 30s in uncompressed PCM 16 bit, 44.1 kHz mono audio files. . Solution Statement . With the above competition requirements in mind, the proposed solution was followed and was undertaken initially within a Pytorch AWS SageMaker notebook instance using Jupyter Notebooks, and further, using a Google Cloud Platform AI Notebook using fastai2 and fastai2 audio libraries , due to the extra credits required for the long training times on a GPU instance: . The data will be downloaded from Kaggle into the chosen platform AWS SageMaker / GCP AI Notebook instance | Exploratory Data Analysis - The dataset will be downloaded such that the file metadata can be extracted, in order to confirm: sample rates, bit-rates, durations, channels (mono/stereo) for each file in order to direct initial signal processing stage and approach towards the dataset splitting. In addition, the statistics of the file labels will be analysed. | Model Development: The fastai2 and fastai2 audio libraries will be installed | The fastai2 audio library will be used for the data processing, in order to convert the audio files into tensor representations of mel-spectrograms on-the-fly, rather than in a separate pre-processing stage. This is a significant benefit of the library in terms of allowing quick experimentation and iteration within the model development over other methods such as converting all audio files to mel-spectrogram images separately. | In-line with the competition rubric, a non-pretrained convolutional neural network (CNN) using the fastai2 library for PyTorch will be developed using state-of-the-art methods and additions. | The model will be trained on the “Noisy Train Set” in a 5-Folds Cross Validation manner, using Sci-Kit Learn’s K-Fold model selection module5, for a shorter period due to the large amount of data. | The results of these 5 models will be used to train on the “Curated Train Set” in the same 5-Folds Cross Validation manner as the noisy train set, but for a longer period. | Test-Time-Augmentation (TTA) will be used to gain averaged predictions from all 5 final models on the test set. The predictions will be submitted as a Late-Submission for the analysis of the results. | This will be repeated, with tweaks to the model augmentations in order to try to improve the results iteratively. | . | Metrics . Due to the advancement of multi-label audio classification in recent years, a simple multi-label accuracy metric was not used within the Kaggle competition, as performances of the systems can easily exceed 95% within a few epochs of training. . As such, the competition used label-weighted label-ranking average precision (a.k.a lwl-rap) as the evaluation metric. The basis for the metric, the label-ranking average precision algorithm, is described in detail within the Sci-Kit Learn implementation6. The additional adaptations of the metric are to provide the average precision of predicting a ranked list of relevant labels per audio file, which is a significantly more complex problem to solve than a standard multi-label accuracy metric. The overall score is the average over all the labels in the test set, with each label having equal weight (rather than equal weight per test item), as indicated by the “label-weighted” prefix. This is defined as follows7: lwlrap=1∑s∣C(s)∣∑a∑eϵ C(s)Prec(s,c)lwlrap = frac{1}{ sum_{s} left | C(s) right |} sum_{a} sum_{e epsilon C(s)}Prec(s,c)lwlrap=∑s​∣C(s)∣1​∑a​∑eϵ C(s)​Prec(s,c) where $Prec(s,c)$ is the label-ranking precision for the list of labels up to class ccc and the set of ground-truth classes for sample sss is C(s)C(s)C(s). ∣C(s)∣|C(s)|∣C(s)∣ is the number of true class labels for sample sss. . The Kaggle competition provides a Google Colab example implementation8. . II. Analysis . Data Exploration . The datasets were downloaded from Kaggle using the Kaggle API and analysed within a Jupyter Notebook. . The first stage of the process was to understand the dataset more fully. Fortunately, due to being a Kaggle Competition dataset it was well documented and clean in terms of organization. . Downloading the dataset was undertaken using guidance given within the Kaggle Forums9 directly into the SageMaker/GCP Instance storage for easy access. . The files were then unzipped for the EDA. For further details, please see the notebook directly. . Pandas and Pandas Profiling . In order to undertake the analysis of the data, the numerical data analysis packages Pandas and Pandas Profiling were used. . Pandas Profiling10 is an extremely useful add-on package to Pandas, which creates HTML profile reports directly from Pandas DataFrames quickly and easily. From the provided .csv files file category labels were analysed and, in addition, the audio file meta-data was extracted (i.e. sample rates, bit-rates, durations, number of channels). . . ​ Fig 1. An example Pandas DataFrame of extracted audio file info . Using these two packages the following was found: . Curated Train Data . For the Curated Train dataset, it was found that the bit-rate was a constant 16bits, the channels a constant 1 (mono), constant sample rate of 44100kHz and that there were 213 different tagging combinations of the 80 audio labels over the total file count (4964 files): . . ​ Fig 2. Pandas Profiling for the Curated Train Data set . In terms of the file durations, the average file length was 7.63 seconds and the files ranged between just over 0 and 30 seconds long, with the lengths predominantly in the 0-5 seconds length range. This will affect the mel-spectrogram processing of the data, i.e. we will need to ensure a sufficient amount of both the longer and smaller audio files are taken, in order for the feature learning of the CNN to be accurate. . . ​ Fig 3. Pandas Profiling information for the audio file durations . Noisy Train Data . As with the Curated dataset, with the Noisy Train dataset it was found that the bit-rate was a constant 16bits, the channels a constant 1 (mono), constant sample rate of 44100kHz. However, in this dataset there were 1168 different tagging combinations of the 80 audio labels over the total file count (19815 files): . . ​ Fig 4. Pandas Profiling for the Noisy Train dataset . The Noisy Train dataset average file length was significantly longer on average than the Curated set at 14.6s long, however, the files ranged between 1 and 16 seconds long. There is therefore a significant difference in terms of length between the two datasets. . . ​ Fig 5. Pandas Profiling information for the audio file durations . In addition, as the name implies, the Noisy Train set files have a significantly higher noise floor than the Curated Train set due to the provenance of the files. . Data Visualisation . The following figure clearly illustrates the differences between the difference in durations of audio files between the two datasets: . . ​ Fig 6. Train vs Noisy dataset durations (x-axis = seconds) . Therefore, in the development of the model the following factors will need to be considered: . Noise floor differences between the curated and noisy train set will affect how the signals are clipped to shorter lengths to feed into the CNN. | The average lengths also have a high range of values both over the the individual datasets and between the curated and noisy set, we will need to ensure the main recorded features corresponding to the file labels of each recording are kept within any clipped sections to produce the mel-spectrograms. | . Algorithms and Techniques . Mel-Spectrograms . This signal processing stage will involve trimming (to ensure uniform duration) in order be converted to uniform length log-mel-spectrogram representations of the audio. A log-mel-spectrogram is a spectrogram representation of the audio i.e. a frequency-domain representation based on the Fourier Transform, with x-axis = time, y axis = frequency and colour depth/pixel value = relative sound intensity, which has been has been converted to the Mel scale on the y-axis by a non-linear transform in order to be more representative of the highly non-linear magnitude and frequency sensitivities of the human ear11. The chosen settings will be discussed and shown further in the Data Preprocessing section. . . ​ Fig 7. Conversion from Waveform to Mel-spectrogram representation . Convolutional Neural Network (CNN) . The length uniformity of the audio clips in is important, as it allows 2D tensors of the mel-spectrograms to be fed in batches into a CNN. The model variety used was as follows, based on the state of the art findings of the fastai community12 and other research described below. The model and architecture used the following settings: . Architecture: fastai2’s XResNet50 based on the Bag of Tricks13 research paper which includes tweaks to the optimisation methods for higher performance. ResNets use skip connections in order to allow propagation of information more deeply into the architecture, giving significant speed improvements for deeper networks and allowing the gradient descent to backpropagate through the network which aids in increasing training accuracy. This has further been augmented in the Bag of Tricks paper, whereby the residual block convolutional layers have been re-arranged such that further efficiency gains are made. . | Activation Function: Mish14 which has been shown to provide performance improvements over the standard ReLU activation function due to its smoothing of the activations rather than the cut-off of the ReLU function for values below 0. | Optimizer Function: Ranger which is a combination of the RAdam15 and Lookahead16 optimizer functions. These functions work as a searching pair, whereby one learner goes ahead of the other to explore the function topography, such that traps involving local minima can be avoided. | Layer tweaks: Self-Attention Layers17 | Replacing Max Pooling Layers with “MaxBlurPool” layers for better generalization | Flat-Cosine decay learning rate scheduling | . K-Folds Validation . Sci-Kit Learn’s KFolds Validation function was used to split the datasets into 5 folds, to allow all of the available data to be used in the training and to further allow the 5 created models to give ensembled predictions on the Test set, which provides a significant performance improvement over a single model. . MixUP . MixUp, whereby two spectrograms are combined to form a third, was also used during the longer Curated Training Set procedure. Detailed further below. . Test-Time Augmentation (TTA) . In addition to the methods outlined above, Test-Time augmentations were applied to the test set, such that the data transformations were used as part of the testing procedure in order to give a further performance boost. . Benchmark . The Baseline performance for the Kaggle Competition was set at 0.53792 which provided a minimum target. The winner18 of the competition acheived 0.75980, which provided the upper target. The details of the winning model and training method can be found on the linked GitHub page, but for brevity, the basic details of the system from the GitHub repo, were as follows: . Log-scaled mel-spectrograms | CNN model with attention, skip connections and auxiliary classifiers | SpecAugment, Mixup augmentations | Hand relabeling of the curated dataset samples with a low score | Ensembling with an MLP second-level model and a geometric mean blending | . Up next . In the next Part 2 of this blog series, we will look at the methodology and implementation of training the model and improving it iteratively. . http://dcase.community/challenge2020/index &#8617; . | https://github.com/rbracco/fastai2_audio &#8617; . | https://www.kaggle.com/c/freesound-audio-tagging-2019/overview ↩ &#8617; &#8617;2 . | Learning Sound Event Classifiers from Web Audio with Noisy Labels - Fonseca et al. 2019 https://arxiv.org/abs/1901.01189 &#8617; . | https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html &#8617; . | https://scikit-learn.org/stable/modules/model_evaluation.html#label-ranking-average-precision) &#8617; . | Fonseca et al. - Audio tagging with noisy labels and minimal supervision. In Proceedings of DCASE2019 Workshop, NYC, US (2019). URL: https://arxiv.org/abs/1906.02975 ↩ ↩ ↩ ↩ ↩ &#8617; . | (https://colab.research.google.com/drive/1AgPdhSp7ttY18O3fEoHOQKlt_3HJDLi8) &#8617; . | https://www.kaggle.com/c/deepfake-detection-challenge/discussion/129521) &#8617; . | https://github.com/pandas-profiling/pandas-profiling &#8617; . | Computational Analysis of Sound Scenes and Events, pg. 22 - Virtanen et al. &#8617; . | https://github.com/muellerzr/Practical-Deep-Learning-for-Coders-2.0/blob/master/Computer%20Vision/04_ImageWoof.ipynb &#8617; . | He, Tong, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. 2018. “Bag of Tricks for Image Classification with Convolutional Neural Networks.” CoRR abs/1812.01187 &#8617; . | Misra, Diganta. 2019. “Mish: A Self Regularized Non-Monotonic Neural Activation Function.” &#8617; . | Liyuan Liu et al. 2019 - On the Variance of the Adaptive Learning rate and Beyond &#8617; . | Zhang, Lucas Hinton, Ba - Lookahead Optimizer: k steps forward, 1 step back &#8617; . | Han Zhang, Ian Goodfellow, Dimitris Metaxas, Augustus Odena 2018 - Self-Attention Generative Adversarial Networks &#8617; . | https://github.com/lRomul/argus-freesound &#8617; . |",
            "url": "https://mikful.github.io/mike-blog/2020/06/04/Udacity-Capstone-Part-1-Definition-and-Data.html",
            "relUrl": "/2020/06/04/Udacity-Capstone-Part-1-Definition-and-Data.html",
            "date": " • Jun 4, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Test",
            "content": "Test .",
            "url": "https://mikful.github.io/mike-blog/2020/06/04/Test.html",
            "relUrl": "/2020/06/04/Test.html",
            "date": " • Jun 4, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://mikful.github.io/mike-blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://mikful.github.io/mike-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://mikful.github.io/mike-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}