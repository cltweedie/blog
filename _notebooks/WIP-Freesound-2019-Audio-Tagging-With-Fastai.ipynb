{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Multi-Label Audio Tagging With Fastai2 Audio\"\n",
    "> \"A Udacity ML Engineer Nanodegree Capstone Projec: Using the in-development fastai2 audio library on the Kaggle Freesound 2019 Competition\"\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: false\n",
    "- comments: true\n",
    "- categories: [audio, deep learning, SageMaker, GCP]\n",
    "- image: images/some_folder/your_image.png\n",
    "- hide: false\n",
    "- search_exclude: true\n",
    "- metadata_key1: metadata_value1\n",
    "- metadata_key2: metadata_value2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "**** \n",
    "\n",
    "\n",
    "Sections:\n",
    "\n",
    "Remember to fully reference and give thanks to references\n",
    "\n",
    "* Intro to Competition\n",
    "* Intro to Fastai2 Audio\n",
    "* Downloading the Data\n",
    "* EDA\n",
    "* Audio Data intro\n",
    "* Chosen Spectrogram settings and Augmentations\n",
    "* 2-stage Training Method - 2-Stage Kfold, Model details, learning rates\n",
    "\n",
    "    - why good: \n",
    "    - used 1/5 training time of high scorers (approximately)\n",
    "    - No time-consuming pre-processing \n",
    "    - multi-label accuracy within 98% in a few epochs on simple accuracy_multi metric\n",
    "* Results  \n",
    "* What's next \n",
    "    - self supervised learning?\n",
    "    - improving on model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Listening\n",
    "\n",
    "The sub-field of Machine Learning known as Machine Listening is a burgeoning area of research using signal processing for the automatic extraction of information from sound by a computational analysis of audio. There are many different areas of research within this field as demonstrated by the latest Detection and Classification of Acoustic Scenes and Events (DCASE) 2020 Challenge, a machine learning challenge dedicated to the research and development of new methods and algorithms. These include:\n",
    "\n",
    "* Acoustic Scene Classification\n",
    "* Sound Event Detection and Localization\n",
    "* Sound Event Detection and Separation in Domestic Environments\n",
    "* Urban Sound tagging\n",
    "* Automated Audio Captioning\n",
    "\n",
    "As an acoustic engineer, I am extremely intrigued by this new field. Recent developments in machine learning algorithms have allowed significant progress to be made within this area, with the potential applications of the technology being wide and varied and meaning the tools could prove to be extremely useful for the acoustic practitioner amongst many other uses.\n",
    "\n",
    "The in-development user-contributed fast.ai2 Audio library {% fn 1 %} inspired me to undertake the development of a deep learning audio-tagging system for my Udacity Capstone project, as described in this blog post.\n",
    "\n",
    "\n",
    "\n",
    "{{ '[fast.ai 2 audio](https://github.com/rbracco/fastai2_audio)'  | fndetail: 1 }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freesound 2019 Kaggle Competition\n",
    "\n",
    "**Problem**\n",
    "\n",
    "The Freesound Audio Tagging 2019 {% fn 2 %} Kaggle Competition provides the basis for my research project. \n",
    "\n",
    "The challenge is to develop a system for the automatic classification of multi-labelled audio files within 80 categories, that could potentially be used for automatic audio/video file tagging and/or real-time sound event detection with noisy or untagged audio data. This has historically been investigated in a variety of ways:\n",
    "\n",
    "* Conversion of audio to mel-spectrogram images fed into CNNs\n",
    "* End-to-End Deep learning\n",
    "* Custom architectures involving auto-encoders\n",
    "* Features representation transfer learning with custom architectures and Google's Audioset\n",
    "\n",
    "**Dataset**\n",
    "\n",
    "In addition, the classification of weakly labelled data from large-scale crowdsourced datasets provides a further problem for investigation[^2]. The dataset comprises the following elements:\n",
    "\n",
    "\n",
    "The dataset used in the challenge is called FSDKaggle2019[^1] and was collected by members of Freesound (a Creative Commons Licensed sound database from the Music Technology Group of Universitat Pempeu Fabra, Barcelona) and Google Research's Machine Perception Team[^2].\n",
    "\n",
    "- Freesound Dataset ([FSD](https://annotator.freesound.org/fsd/)): a dataset being collected at the [MTG-UPF](https://www.upf.edu/web/mtg) based on [Freesound](https://freesound.org/) content organized with the [AudioSet Ontology](https://research.google.com/audioset////////ontology/index.html) and manually labelled by humans.\n",
    "- The soundtracks of a pool of Flickr videos taken from the [Yahoo Flickr Creative Commons 100M dataset (YFCC)](http://code.flickr.net/2014/10/15/the-ins-and-outs-of-the-yahoo-flickr-100-million-creative-commons-dataset/) which are automatically labelled using metadata from the original Flickr clips. These items therefore have significantly more label noise than the Freesound Dataset items.\n",
    "\n",
    "The data comprises 80 categories labelled according to Google's Audioset Ontology [^3] with ground truth labels provided at the clip level. The clips range in duration between 0.3 to 30s in uncompressed PCM 16 bit, 44.1 kHz mono audio files.\n",
    "\n",
    "\n",
    "**Metric**\n",
    "\n",
    "The problem is clearly quantifiable in that a number of accuracy metrics could be used to quantify the accuracy of the model's predictions, however, the competition used label-weighted-label-ranked-precision (lwl-rap, or 'lol-rap') as the metric, as the advancement of audio multi-labelling has acheived >95% accuracy with ease in recent years. This will be discussed in further detail below.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* Intro - from proposal\n",
    "    * Problem - tagging data using noisy datasets and small curated set\n",
    "* Multi-label - lwl-wrap (more on that later)\n",
    "* Data sets - Curated and Noisy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "{{ '[Kaggle Freesound Audio Tagging 2019 Competition](https://www.kaggle.com/c/freesound-audio-tagging-2019/overview)'  | fndetail: 2 }}\n",
    "\n",
    "    * Difference between audio and other data*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA - WIP\n",
    "\n",
    "The first stage of the process was to understand the dataset more fully. Fortunately, due to being a Kaggle Competition dataset it was well documented and clean in terms of organization.\n",
    "\n",
    "Downloading the dataset was undertaken using guidance given within [the following link](https://www.kaggle.com/c/deepfake-detection-challenge/discussion/129521) directly into the SageMaker/GCP Instance storage for easy access.\n",
    "\n",
    "The files were then unzipped for the EDA. For further details, please see the [notebook directly](https://github.com/mikful/udacity-mlend-capstone/blob/master/nbs_final/eda.ipynb).\n",
    "\n",
    "\n",
    "**Pandas and Pandas Profiling**\n",
    "\n",
    "In order to undertake the analysis of the data, Pandas and Pandas Profiling were used. \n",
    "\n",
    "[Pandas Profiling](https://github.com/pandas-profiling/pandas-profiling) is an extremely useful add-on package to Pandas, which creates HTMl profile reports directly from Pandas DataFrames quickly and easily.\n",
    "\n",
    "Using these two packages the following was found:\n",
    "\n",
    "\n",
    "**Curated Data**\n",
    "\n",
    "\n",
    "**Noisy Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing the Model - To Do\n",
    "\n",
    "Although the course was taught using SageMaker, for the training an AI Notebook instance on GCP was used, as the author had free credits available and the GPU training times were likely to be significantly longer than possible with the remaining SageMaker credit from the course.\n",
    "\n",
    "* Audio Data intro\n",
    "* Chosen Spectrogram settings and Augmentations\n",
    "* 2-stage Training Method - 2-Stage Kfold, Model details, learning rates\n",
    "\n",
    "    - why good: \n",
    "    - used 1/5 training time of high scorers (approximately)\n",
    "    - No time-consuming pre-processing \n",
    "    - multi-label accuracy within 98% in a few epochs on simple accuracy_multi metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Results - To Do\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Model - To Do\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next...\n",
    "\n",
    "\n",
    "* Deploy a model using SageMaker / GCP for inference of user-uploaded audio.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thanks\n",
    "\n",
    "Thanks to fastai and the fastai2-audio and fastpages communities whose amazing efforts and incredible generosity have made this learning journey possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
