{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Orchid Genus Classifier App Using fastai, Render and Flutter - Part 2\n",
    "\n",
    "- toc: false\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Mike Fuller\n",
    "- categories: [fastai, jupyter, render, flutter]\n",
    "- image: images/orchid.jpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MyeyimGkcYJf"
   },
   "source": [
    "In this blog series I'll show how to train an orchid genus classifier using [fastai](https://www.fast.ai/), deploy this to [Render](https://render.com/) and create a Flutter app for the front-end. This will be done in two parts:\n",
    "\n",
    "1. [Dataset Collection and fastai Image Classifier Training](https://mikful.github.io/blog/fastai/jupyter/render/flutter/2020/09/15/orchid-classifier-fastai-render-flutter.html)\n",
    "2. Render Deployment and Flutter App\n",
    "\n",
    "This is part 2, please see the associated [github repo for render](https://github.com/mikful/orchid-classifier-render) and [github repo for flutter](https://github.com/mikful/orchid-classifier-flutter) for further implementation details.\n",
    "\n",
    "## Render Deployment\n",
    "\n",
    "Now that we've trained our model, we need to deploy our fastai Learner to a dockerized environment, such that we can perform inference on new images. The fastai course v3 had a [starter package](https://github.com/render-examples/fastai-v3) for Render that we'll update to work with the latest fastai library (version 2) and also for fastapi in this case. (Note: the fastbook and fastai website now give different options, such as a binder-hosted jupyter notebook for simple inference, or [seemeai](https://course.fast.ai/deployment_seeme_ai), although I find render and heroku are still very good options and more flexible than binder.)\n",
    "\n",
    "### requirements.txt\n",
    "\n",
    "First, let's update our package dependencies (that we listed within our Notebook environment in Part 1 of this blog series). Be sure to use the CPU versions of PyTorch versions within the Notebook/Colab Environment for the docker deployment, not the larger Cuda enabled wheels as for the inference we do not require GPU usage. You can ensure to set these using `+cpu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch==1.6.0+cpu \n",
    "torchvision==0.7.0+cpu\n",
    "-f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These and all other dependencies in the requirements.txt file will be installed when the Dockerfile builds the container image using the following line:\n",
    "\n",
    "``RUN pip install --upgrade -r requirements.txt``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### server.py\n",
    "\n",
    "Now we need to update the server.py file that contains the app. Firstly, the original [Starlette](https://www.starlette.io/) asyncio library was replaced by the [fastapi](https://fastapi.tiangolo.com/) library, which is a little clearer in its syntax and easier to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the location of our exported fastai Learner. In part 1, we saw how to upload this to a Google Cloud Storage bucket - we need to ensure the permissions on the file are set such that we can download this, which can be done in the file settings in the bucket (for more details [see here](https://cloud.google.com/storage/docs/access-control/making-data-public)).\n",
    "\n",
    "Then we set this within the script:\n",
    "\n",
    "```python\n",
    "path = Path(__file__).parent\n",
    "export_file_url = 'https://storage.googleapis.com/fastai-export-bucket/export.pkl' # google cloud bucket / file url\n",
    "export_file_name = 'export.pkl'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference**\n",
    "\n",
    "The main functions within the server.py file are fairly self evident, in that it downloads the `export.pkl` file and then loads it into a new fastai `Learner`.\n",
    "\n",
    "For the inference, the main function is contained within the following code section that takes the image bytes from the post request and performs the inference on it. The prediction is then returned in the JSON response.\n",
    "\n",
    "```python\n",
    "@app.post(\"/analyze\")\n",
    "async def analyze(file: bytes = File(...)):\n",
    "    img_bytes = BytesIO(file)\n",
    "    prediction, idx, preds = learn.predict(img_bytes.getvalue())\n",
    "    return JSONResponse({'result': str(prediction)})\n",
    "```\n",
    "\n",
    "**HTML**\n",
    "\n",
    "Note that within the repo is also the css and [html file](https://github.com/mikful/orchid-classifier-render/blob/master/app/view/index.html) for a webapp, that can be used as the interface to perform the inference if desired, instead of the Flutter app. However, in this case, I wanted to make a full Android app, not a webapp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flutter App\n",
    "\n",
    "Flutter is a cross-platform app development solution, with near native application speeds. \n",
    "\n",
    "Thanks to the code given at: https://github.com/dnmanveet/Fruit_classifier_app I could successfully connect my Render backend to a working Android Flutter App.\n",
    "\n",
    "It was really as simple as changing the base render deployment location within [main.dart](https://github.com/mikful/orchid-classifier-flutter/blob/6fa185dd2a61e5911d67be69ee6c38c8d46c8ba7/orchid-classifier-flutter/lib/main.dart).\n",
    "\n",
    "```python\n",
    "    String base =\n",
    "        \"https://orchid-classifier.onrender.com\";\n",
    "```\n",
    "\n",
    "Then, all that remained were some interface and styling tweaks and I had a pretty decent looking basic app for trying out on my own images!\n",
    "\n",
    "\n",
    "![Final App]({{ site.baseurl }}/images/orchid-classifier/app.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Interestingly, in testing the predictions were initially way off, with many errors, however, I decided to try and zoom in on the images to see if the problem was related to the angle and distance of the photos. This resulted in a vast improvement - the model is clearly very sensitive to distance of the photo, or size of the flower relative to the image. \n",
    "\n",
    "<figure>\n",
    "    <div style=\"display:flex\">\n",
    "        <div style=\"flex:1\">\n",
    "            <figure>\n",
    "<img src=\"{{ site.baseurl }}/images/orchid-classifier/cymbidium.jpg\">\n",
    "                <figcaption><center>Original photo = incorrect prediction</center></figcaption>\n",
    "            </figure>\n",
    "        </div>\n",
    "        <div style=\"flex:1\">\n",
    "            <figure>\n",
    "<img src=\"{{ site.baseurl }}/images/orchid-classifier/cymbidium-zoomed.jpg\">\n",
    "                <figcaption><center>Zoomed photo = correct prediction</center></figcaption>\n",
    "            </figure>\n",
    "        </div>\n",
    "    </div>\n",
    "    <figcaption><center>Cymbidium Predictions</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "<figure>\n",
    "    <div style=\"display:flex\">\n",
    "        <div style=\"flex:1\">\n",
    "            <figure>\n",
    "<img src=\"{{ site.baseurl }}/images/orchid-classifier/odont1.jpg\">\n",
    "                <figcaption><center>Original photo = incorrect prediction</center></figcaption>\n",
    "            </figure>\n",
    "        </div>\n",
    "        <div style=\"flex:1\">\n",
    "            <figure>\n",
    "<img src=\"{{ site.baseurl }}/images/orchid-classifier/odont1-zoomed.jpg\">\n",
    "                <figcaption><center>Zoomed photo = correct prediction</center></figcaption>\n",
    "            </figure>\n",
    "        </div>\n",
    "    </div>\n",
    "    <figcaption><center>Odontoglossum Predictions</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "<figure>\n",
    "    <div style=\"display:flex\">\n",
    "        <div style=\"flex:1\">\n",
    "            <figure>\n",
    "<img src=\"{{ site.baseurl }}/images/orchid-classifier/odont2.jpg\">\n",
    "                <figcaption><center>Original photo = incorrect prediction</center></figcaption>\n",
    "            </figure>\n",
    "        </div>\n",
    "        <div style=\"flex:1\">\n",
    "            <figure>\n",
    "<img src=\"{{ site.baseurl }}/images/orchid-classifier/odont2-zoomed.jpg\">\n",
    "                <figcaption><center>Zoomed photo = correct prediction</center></figcaption>\n",
    "            </figure>\n",
    "        </div>\n",
    "    </div>\n",
    "    <figcaption><center>Odontoglossum Predictions</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "On photos taken face-on, with good definition, the results were of much higher accuracy:\n",
    "\n",
    "\n",
    "![Masdevallia]({{ site.baseurl }}/images/orchid-classifier/masdevallia.jpg)\n",
    "\n",
    "\n",
    "As such, it will be necessary to build in a zoom function for cropping and centring the flower head before inference. In addition, on some rarer varieties of orchid species I tried there was a clear error, as the rarer species may not have been covered in the tst data, however, I'm surprised at how well this works in general and will be useful practical tool (with a bit of checking with an expert ofcourse!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "All in all, this was a fun little project that appears to be giving quite decent results in the real world. Improvements and next steps for the app development, would be the following:\n",
    "\n",
    "* A zoom function to centre the flower head before the inference is made\n",
    "* Create an S3 storage bucket to store all the uploaded photos for later further training of the model\n",
    "* Create an option for the user to confirm the prediction as correct or incorrect, and input the real flower genus if so\n",
    "\n",
    "A may promising approach may be to use a multi-model network, that contains both descriptive text information and images, such as the NTS-net described in the paper [Learning to Navigate for Fine-grained Classification](https://arxiv.org/abs/1809.00287). Perhaps even dimensional measurements would be useful for the classificaton process and would help to account for the potential subtle difference between the genera.\n",
    "\n",
    "I may update this blog series with a part 3 at another time including these items if I can incorporate them.\n",
    "\n",
    "I hope this series may be of use, and if anyone reading this has any queries regarding the training or deployment, or any comments in general, please let me know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMGcPDJKCsAsIwPJiv0hdZN",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1ZS11Pkfat5EwOKqeR3_NkhhMT2q3DZUE",
   "name": "fastai2 - 19001 - Orchid Classifier v2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
